{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, Conv1D, Conv2D, Layer, \\\n",
    "LSTM, GlobalAveragePooling1D, subtract, add, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from metrics import max_absolute_error, mean_absolute_error\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import pandas as pd\n",
    "from utils import data_scale, sequentialize, split_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import mpld3\n",
    "mpld3.enable_notebook()\n",
    "# TODO move metrics and utils here\n",
    "# TODO do we need mpld3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general hyperparameters\n",
    "training_data_ratio = 0.9\n",
    "sample_length = 128\n",
    "step_ahead = 32\n",
    "batch_size = 32\n",
    "dropout_ratio = 0.2\n",
    "\n",
    "basin = \"reno\"\n",
    "river_station = \"casalecchio-chiusa\"\n",
    "rain_station = \"vergato\"\n",
    "start_year = 2006\n",
    "end_year = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubleDense(units,x):\n",
    "    x = Dense(units)(x)\n",
    "    x = Dense(units)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_input(sample_lenght, training_data_ratio, look_ahead):\n",
    "    dataset_level = pd.read_csv(\"data/level/{}/{}/{}-{}.csv\".format(basin, river_station, start_year, end_year),\n",
    "                                parse_dates=[0], index_col=0)\n",
    "    dataset_rain = pd.read_csv(\"data/rain/{}/{}/{}-{}.csv\".format(basin, rain_station, start_year, end_year),\n",
    "                               parse_dates=[0], index_col=0)\n",
    "\n",
    "    # (to save space in file) create new column as index\n",
    "    dataset_level['rain'] = dataset_rain.values\n",
    "\n",
    "    #rolling averange on rain and level to remove noise\n",
    "    dataset_level = dataset_level.rolling(24).mean().fillna(method='bfill')\n",
    "\n",
    "    dataset_level = dataset_level.dropna()\n",
    "\n",
    "    dataset_level['level'], _ = data_scale(dataset_level['level'].values)\n",
    "    dataset_level['rain'], _ = data_scale(dataset_level['rain'].values)\n",
    "    dataset_level['dayofyear'], _ = data_scale(dataset_level.index.dayofyear.values)\n",
    "\n",
    "    dates = dataset_level.index.values\n",
    "\n",
    "    dataset = dataset_level.values\n",
    "\n",
    "    x_dataset, y_dataset = sequentialize(dataset, sample_lenght, look_ahead)\n",
    "\n",
    "    train_x, val_x = split_dataset(x_dataset, training_data_ratio)\n",
    "    train_y, val_y = split_dataset(y_dataset, training_data_ratio)\n",
    "\n",
    "\n",
    "    val_dates = dates[-val_y.shape[0]:]\n",
    "\n",
    "    return train_x, train_y, val_x, val_y, val_dates\n",
    "\n",
    "\n",
    "train_x, train_y, val_x, val_y, val_dates = \\\n",
    "    load_input(sample_length, training_data_ratio, step_ahead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LSTM Hyperparameters\n",
    "epoch = 1\n",
    "memory = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_layer_wrapper(units,x):\n",
    "    x = tf.expand_dims(x,axis=-1)\n",
    "    x = LSTM(units=memory)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dense(units)(x)\n",
    "    return x\n",
    "\n",
    "in_seq = Input(shape=(sample_length, 3))\n",
    "timeof = doubleDense(sample_length,in_seq[:,:,2])\n",
    "timeof2 = doubleDense(sample_length,in_seq[:,:,2])\n",
    "river = LSTM_layer_wrapper(step_ahead,subtract([in_seq[:,:,0],timeof]))\n",
    "rain = LSTM_layer_wrapper(step_ahead,subtract([in_seq[:,:,1],timeof2]))\n",
    "out = add([river,rain])\n",
    "lstm_model = Model(inputs=[in_seq], outputs=out)\n",
    "lstm_model.compile(loss='mse', optimizer='adam', metrics=[max_absolute_error, mean_absolute_error])\n",
    "lstm_model.build(input_shape=(train_x.shape))\n",
    "plot_model(lstm_model,show_layer_names=False, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='loss', min_delta=0.0001, patience=2, restore_best_weights=True)\n",
    "history = lstm_model.fit(train_x, train_y,\n",
    "                            validation_data=(val_x,val_y),\n",
    "                            epochs=epoch, batch_size=batch_size, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_dimension = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(units,x):\n",
    "    x = Dense(inner_dimension)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dense(units)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    return x\n",
    "\n",
    "in_seq = Input(shape=(sample_length, 3))\n",
    "timeof = doubleDense(sample_length,in_seq[:,:,2])\n",
    "timeof2 = doubleDense(sample_length,in_seq[:,:,2])\n",
    "river = encoding(step_ahead,subtract([in_seq[:,:,0],timeof]))\n",
    "rain = encoding(step_ahead,subtract([in_seq[:,:,1],timeof2]))\n",
    "out = add([rain,river])\n",
    "ff_model = Model(inputs=in_seq, outputs=out)\n",
    "ff_model.compile(loss='mse', optimizer='adam', metrics=[max_absolute_error, mean_absolute_error])\n",
    "ff_model.build(input_shape=(train_x.shape))\n",
    "plot_model(ff_model, show_layer_names=False, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='loss', min_delta=0.00001, patience=3, restore_best_weights=True)\n",
    "history = ff_model.fit(train_x, train_y,\n",
    "                            validation_data=(val_x,val_y),\n",
    "                            epochs=epoch, batch_size=batch_size, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tcn import TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_encoding = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TCN_layer(units,x):\n",
    "    x = tf.expand_dims(x,axis=-1)\n",
    "    x = TCN(inner_encoding, kernel_size =2, dilations=[1,2,4], use_skip_connections=True)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dense(units)(x)\n",
    "    return x\n",
    "\n",
    "in_seq = Input(shape=(sample_length, 3))\n",
    "timeof = doubleDense(sample_length,in_seq[:,:,2])\n",
    "timeof2 = doubleDense(sample_length,in_seq[:,:,2])\n",
    "river = TCN_layer(step_ahead,subtract([in_seq[:,:,0],timeof]))\n",
    "rain = TCN_layer(step_ahead,subtract([in_seq[:,:,1],timeof2]))\n",
    "out = add([rain,river])\n",
    "tcn_model = Model(inputs=in_seq, outputs=out)\n",
    "tcn_model.compile(loss='mse', optimizer='adam', metrics=[max_absolute_error, mean_absolute_error])\n",
    "tcn_model.build(input_shape=(train_x.shape))\n",
    "plot_model(tcn_model, show_layer_names=False, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='loss', min_delta=0.00001, patience=3, restore_best_weights=True)\n",
    "history = tcn_model.fit(train_x, train_y,\n",
    "                            validation_data=(val_x,val_y),\n",
    "                            epochs=epoch, batch_size=batch_size, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformerRegression(units,x):\n",
    "    query = Dense(8)(x)\n",
    "    value = Dense(8)(x)\n",
    "    key = Dense(8)(x)\n",
    "    query, value, key = [tf.expand_dims(x,axis=1) for x in [query,value,key]]\n",
    "    x = Attention()([query,value,key])\n",
    "    x = LayerNormalization()(x)\n",
    "    x = GlobalAveragePooling1D(data_format='channels_last')(x)\n",
    "    x = Dense(units)(x)\n",
    "    return x\n",
    "\n",
    "in_seq = Input(shape=(sample_length, 3))\n",
    "timeof = doubleDense(sample_length,in_seq[:,:,2])\n",
    "timeof2 = doubleDense(sample_length,in_seq[:,:,2])\n",
    "river = transformerRegression(step_ahead,subtract([in_seq[:,:,0],timeof]))\n",
    "rain = transformerRegression(step_ahead,subtract([in_seq[:,:,1],timeof2]))\n",
    "out = add([rain,river])\n",
    "transformer_model = Model(inputs=in_seq, outputs=out)\n",
    "transformer_model.compile(loss='mse', optimizer='adam', metrics=[max_absolute_error, mean_absolute_error])\n",
    "transformer_model.build(input_shape=(train_x.shape))\n",
    "plot_model(transformer_model, show_layer_names=False, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='loss', min_delta=0.00001, patience=3, restore_best_weights=True)\n",
    "history = transformer_model.fit(train_x, train_y,\n",
    "                            validation_data=(val_x,val_y),\n",
    "                            epochs=epoch, batch_size=batch_size, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_level_trasformer = transformer_model.predict(val_x)\n",
    "predict_level_tcn = tcn_model.predict(val_x)\n",
    "predict_level_ff = ff_model.predict(val_x)\n",
    "predict_level_lstm = lstm_model.predict(val_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
